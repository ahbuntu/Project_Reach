\section{Introduction}
While the rising popularity and adoption of large screen mobile phones presents users with a larger canvas for content consumption, it also poses several significant usability issues. The most important one being the ergonomic mismatch that these larger devices pose for single-handed use. Most of today's mobile interactions are centered on quick, short and intermittent usage patterns. Hence, single-handed interaction with mobile devices is of importance, since it supports these kinds of usage patterns. All device manufacturers who make large screen mobile devices have implemented ways to enable single-handed use. Apple's larger phones allow the user to double tap the home button to quickly pull the entire interface closer to the bottom edge of the screen, Samsung, allows users to snap windows to different corners of the screen to allow for one handed access. However, these methods add an extra step that the user must go through in order to interact with the device, thus breaking the continuity of the interaction.
\par
With REACH we propose to solve the above mentioned problem by appending sensor systems to the phone which can detect when the user is trying to reach the far corners of the phone and affect changes in the UI accordingly. As a bonus, project reach also explores the use of this appended sensor framework to enable additional interaction on mobile devices.
\par
Although prior work has been done on sensing grip, our project aims to use this information for a very specific purpose. We demonstrate that it is possible to use our hardware prototype to accurately determine the type of grip that the user is employing. This is achieved through the use of machine learning algorithms which prove to be quite robust and adept at performing this type of classifications.
