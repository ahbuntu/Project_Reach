\section{Related Work}
Many researchers have suggested that the devices should be intelligent enough to detect user's situation for better support as in \cite{Johnson:1998:UMIM} and \cite{Schmidt:1999:AIC}. For instance, \emph{ability based design} aims to find the best match between the ability of the users and the interfaces \cite{Wobbrock:2011:adcpe}. There are also researches to recognize the activity of users on devices (also known as \emph{activity recognition}). Choudhuri \emph{et al.} \cite{Choudhury:2008:MSP} built a wearable device with sensors to detect the activity of the users. In \cite{Van:2000:WhatShallWe}, Laerhoven used an accelerometer in a phone to recognize different motions of walking, climbing stairs, \emph{etc}. Schmidt \emph{et al.} \cite{Schmidt:1999:AIC} also used accelerometer but to detect both the user movement and the place of the device itself whether it is in the hand or on a table or in a suitcase. GripSense \cite{Goel:2012:gripsense} used gyroscope and vibration motor to classify the user's touches based on the pressure on the screen. There is also many studies in the context of detecting hand postures. Harrison \emph{et al.} \cite{harrison:1998:squeeze} and Kim \emph{et al.} \cite{kim:2006:hand} used touch sensors to detect the pattern of user's grips on mobiles. Furthermore, Taylor and Bove \cite{taylor:2009:graspables} used accelerometers to improve the detection of the changes in the grip dynamically.
\par
Many researchers also studied hand posture on devices to make them more intelligent and interactive to the situations caused by posture. For instance, Wobbrock \emph{et al.} \cite{Wobbrock:2008:PHP} studied different hand postures and measured the finger performance with mobile devices. Holz \emph{et al.} \cite{Holz:2011:UT} have evaluated systematic error in selecting the target with finger touch. Researchers \cite{Hinckley:2011:SST,Weberg:2001:PBP,Karlson:2005:ALT} also found that mobile interfaces are designed for double-handed operation although users may prefer to use one single hand. Karlson \emph{et al.} \cite{Karlson:2006:usm} studied those interfaces and evaluated the performance of thumb mobility on those interfaces. Azenkot and Zhai \cite{Azenkot:2012:TBD} showed that different hand postures lead to different touch patterns, thus, effect the performance of typing on mobile devices. AppLens and LaunchTiles \cite{Karlson:2005:ALT} designed interfaces based on different thumb gestures for one handed interactions.
\par
Fitzmaurice \emph{et al.} \cite{Fitzmaurice:1995:BLF} introduced the idea of ``graspable user interfaces'' where you can control the interface by interacting with a physical object. SqueezeBlock \cite{Gupta:2010:SUV} is an implementation of this idea in which it provides haptic feedback according to the level of ``squashiness'' on a physical object. Wimmer \emph{et al.} \cite{Wimmer:2010:FGS} deployed optical fibers into a surface of device to detect grasping pressure. Harrison \emph{et al.} \cite{harrison:1998:squeeze} used FSRs for squeezing pressure detection. Strachan and Murray-Smith \cite{Strachan:2004:MTI}, used muscle tremor as a form of input to detect pressure on devices by leveraging accelerometer logs.
\par
REACH differs from previous works in that it uses the hardware implementation of force sensors to study users grip patterns on mobile phones. This opens the possibility to detect the user's activity from the pressure he applies on the device in realtime manner. 